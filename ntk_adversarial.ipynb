{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4pUOskgpWyx"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Requires installation of neural-tangents (follow [instructions](https://github.com/google/neural-tangents?tab=readme-ov-file#installation) for your environment)"
      ],
      "metadata": {
        "id": "mhESwJHDHSUL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUJihQBnvGKx"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-dimension\n",
        "!pip install adversarial-robustness-toolbox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDWpI2Oo9Ijd"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOHsNo6Jymgy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import os\n",
        "from random import seed\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import BatchNormalization, LayerNormalization, ReLU\n",
        "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
        "from keras.optimizers.legacy import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import load_mnist\n",
        "\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "import torch\n",
        "\n",
        "from jax import random\n",
        "from jax.example_libraries import optimizers\n",
        "from jax import jit, grad, vmap\n",
        "\n",
        "import jax.numpy as np\n",
        "import functools\n",
        "import neural_tangents as nt\n",
        "from neural_tangents import stax\n",
        "import jax\n",
        "import jaxlib\n",
        "\n",
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.estimators.classification import KerasClassifier, TensorFlowV2Classifier\n",
        "from art.utils import load_mnist, load_cifar10\n",
        "from art.attacks.evasion import SpatialTransformation\n",
        "\n",
        "import skdim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRha_JHWL0cq"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xQb-E3VT_o0"
      },
      "outputs": [],
      "source": [
        "def noisy(image):\n",
        "  row,col,ch = image.shape\n",
        "  gauss = 0.5*numpy.random.randn(row,col,ch)\n",
        "  gauss = gauss.reshape(row,col,ch)\n",
        "  noisy = image + image * gauss\n",
        "  return noisy\n",
        "\n",
        "def add_noise(images):\n",
        "    noisy_images = images.copy()\n",
        "    for i in range(noisy_images.shape[0]):\n",
        "        noisy_images[i] = noisy(noisy_images[i])\n",
        "    return noisy_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaLU07_OLyVd"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = load_mnist()\n",
        "input_shape=(28, 28, 1)\n",
        "\n",
        "x_test_no_noise = x_test.copy()\n",
        "x_test = add_noise(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0acWAS4qRKy"
      },
      "source": [
        "# Dataprep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAnRScJJRKfu"
      },
      "outputs": [],
      "source": [
        "# Select only class 3 and 5 for binary classification\n",
        "\n",
        "label_first = 3\n",
        "label_second = 5\n",
        "\n",
        "select_first = y_train[:, label_first]==1\n",
        "select_second = y_train[:, label_second]==1\n",
        "select_idx = numpy.logical_or(select_first, select_second)\n",
        "x_train = x_train[select_idx, :]\n",
        "y_train = y_train[select_idx][:, [label_first, label_second]]\n",
        "\n",
        "select_first = y_test[:, label_first]==1\n",
        "select_second = y_test[:, label_second]==1\n",
        "select_idx = numpy.logical_or(select_first, select_second)\n",
        "x_test = x_test[select_idx, :]\n",
        "x_test_no_noise = x_test_no_noise[select_idx, :]\n",
        "y_test = y_test[select_idx][:, [label_first, label_second]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXWbEzEIYc8p"
      },
      "source": [
        "## Optional: sampling to overcome potential memory problems of NTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iytThlpYhFA"
      },
      "outputs": [],
      "source": [
        "numpy.random.seed(seed=0)\n",
        "idx = numpy.random.randint(y_train.shape[0], size=10000)\n",
        "x_train = x_train[idx,:]\n",
        "y_train = y_train[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An05IVGwMHNT"
      },
      "source": [
        "# Define the NTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOVKwrDFWdIn"
      },
      "source": [
        "## Jax model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNap1WG33T92"
      },
      "outputs": [],
      "source": [
        "init_fn, apply_fn, kernel_fn = stax.serial(\n",
        "    stax.Conv(4, (5, 5), padding=\"SAME\"),\n",
        "    stax.Relu(),\n",
        "    stax.AvgPool((2, 2)),\n",
        "\n",
        "    stax.Conv(10, (5, 5), padding=\"SAME\"),\n",
        "    stax.Relu(),\n",
        "    stax.AvgPool((2, 2)),\n",
        "\n",
        "    stax.Flatten(),\n",
        "\n",
        "    stax.Dense(16),\n",
        "    stax.Relu(),\n",
        "\n",
        "    stax.Dense(2, W_std=2.**0.5, b_std=0.05)\n",
        "\n",
        "    #no softmax is available: https://github.com/google/neural-tangents/issues/117\n",
        ")\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "_, params = init_fn(key, input_shape=x_train.shape) # -1 at first place?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4V_P84JsHu_"
      },
      "source": [
        "## Computation of NTK kernel\n",
        "\n",
        "Uses empirical_ntk due to memory issues with kernel_fn with CNN;\n",
        "returns covariate that is converted to a single number based on the option in `tensor_to_matrix`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VjrnPwitd4k"
      },
      "outputs": [],
      "source": [
        "def covariate2Number(kernel, tensor_to_matrix=\"det\"):\n",
        "  torch_tensor = torch.tensor(numpy.array(kernel))\n",
        "\n",
        "  if tensor_to_matrix == 'norm':\n",
        "    torch_norm = torch.norm(torch_tensor, p=2, dim=(2,3), keepdim=True)\n",
        "    kernel = numpy.array(torch_norm.reshape(kernel.shape[0], kernel.shape[1]))\n",
        "  elif tensor_to_matrix == 'min':\n",
        "    torch_norm = torch.amin(input=torch_tensor, dim=(2,3))\n",
        "    kernel = numpy.array(torch_norm.reshape(kernel.shape[0], kernel.shape[1]))\n",
        "  elif tensor_to_matrix == 'det':\n",
        "    kernel = np.linalg.det(kernel[:,:])\n",
        "    kernel = kernel.reshape(kernel.shape[0], kernel.shape[1])\n",
        "  return kernel\n",
        "\n",
        "kernel_fn = nt.empirical_ntk_fn(apply_fn, vmap_axes=0, implementation=1, trace_axes=(), diagonal_axes=())\n",
        "kernel_train = kernel_fn(x_train, None, params)\n",
        "kernel_train = covariate2Number(kernel_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvGor2c4MA15"
      },
      "source": [
        "# Train the underlying Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGKEFygF0ANr"
      },
      "outputs": [],
      "source": [
        "SEED = 0\n",
        "\n",
        "def set_seeds(my_seed=SEED):\n",
        "    os.environ['PYTHONHASHSEED'] = str(my_seed)\n",
        "    seed(my_seed)\n",
        "    tf.random.set_seed(my_seed)\n",
        "    numpy.random.seed(my_seed)\n",
        "\n",
        "def set_global_determinism(my_seed=SEED):\n",
        "    set_seeds(my_seed)\n",
        "\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
        "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
        "\n",
        "set_global_determinism(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IILfC0-vs-Cz"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KiwNwUOMGfc"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=4, kernel_size=(5, 5), strides=1, input_shape=input_shape))\n",
        "model.add(ReLU())\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters=10, kernel_size=(5, 5), strides=1))\n",
        "model.add(ReLU())\n",
        "model.add(AveragePooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(16))\n",
        "model.add(ReLU())\n",
        "\n",
        "model.add(Dense(2, activation=\"softmax\"))\n",
        "\n",
        "loss_object = keras.losses.BinaryCrossentropy()\n",
        "optimizer_object = keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "model.compile(loss=loss_object, optimizer=optimizer_object, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RCr9U4EtKwr"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKn_2KgTjgDe"
      },
      "outputs": [],
      "source": [
        "classifier = TensorFlowV2Classifier(model=model,\n",
        "                                    nb_classes=2,\n",
        "                                    input_shape=x_train.shape[1:],\n",
        "                                    loss_object=loss_object,\n",
        "                                    optimizer=optimizer_object,\n",
        "                                    clip_values=(0, 1))\n",
        "\n",
        "\n",
        "\n",
        "classifier.fit(x_train, y_train, batch_size=32, nb_epochs=10, verbose=True)\n",
        "predictions = classifier.predict(x_test)\n",
        "error_test_idx = np.argmax(predictions, axis=1) != np.argmax(y_test, axis=1)\n",
        "accuracy = 100*np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
        "accuracy = round(float(accuracy), 2)\n",
        "print(f\"Accuracy on test examples: {accuracy}%\")\n",
        "\n",
        "extractor = keras.Model(inputs=classifier.model.inputs,\n",
        "                        outputs=[layer.output for layer in model.layers])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdRiHj2hMW0w"
      },
      "source": [
        "# Generate Adversarial Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TCioDRFMcq-"
      },
      "outputs": [],
      "source": [
        "def spatial_transformation_attack(classifier, x_test, y_test):\n",
        "  attack = SpatialTransformation(classifier,\n",
        "                                 max_translation=20,\n",
        "                                 max_rotation=20,\n",
        "                                 num_translations=1,\n",
        "                                 num_rotations=1)\n",
        "  return attack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYpGKyBi4HY0"
      },
      "outputs": [],
      "source": [
        "x_test_to_attack = x_test_no_noise.copy()\n",
        "attack = spatial_transformation_attack(classifier, x_test_to_attack, y_test)\n",
        "n_test_adv_examples = 1000 # desired number of adv examples\n",
        "\n",
        "x_test_adv = attack.generate(x=x_test_to_attack[:n_test_adv_examples, :],\n",
        "                             y=y_test[:n_test_adv_examples,:])\n",
        "y_test_adv = y_test[:n_test_adv_examples,:].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kbbd5zxLseb5"
      },
      "outputs": [],
      "source": [
        "# Evaluate on adversarial test examples\n",
        "\n",
        "feature_layer = 6 # embeddings of this layer are used as features\n",
        "\n",
        "predictions = classifier.predict(x_test_adv)\n",
        "error_idx = np.argmax(predictions, axis=1) != np.argmax(y_test[:n_test_adv_examples,:], axis=1)\n",
        "accuracy = 100*np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[:n_test_adv_examples,:], axis=1)) / len(y_test[:n_test_adv_examples,:])\n",
        "accuracy = round(float(accuracy), 2)\n",
        "print(f\"Accuracy on adversarial test examples: {accuracy}%\")\n",
        "\n",
        "x_train_feat = extractor(x_train)[feature_layer].numpy()\n",
        "print(x_train_feat.shape)\n",
        "\n",
        "# Keep only the erroneous adversarial test examples\n",
        "x_test_adv_sel = x_test_adv.copy()\n",
        "y_test_adv_sel = y_test_adv.copy()\n",
        "x_test_adv_sel = x_test_adv[error_idx, :].copy()\n",
        "y_test_adv_sel = y_test_adv[error_idx, :].copy()\n",
        "x_test_adv_sel_feat = extractor(x_test_adv_sel)[feature_layer].numpy()\n",
        "print(x_test_adv_sel.shape)\n",
        "print(x_test_adv_sel_feat.shape)\n",
        "\n",
        "# Keep only erroneous test examples\n",
        "x_test_sel = x_test[error_test_idx, :].copy()\n",
        "y_test_sel = y_test[error_test_idx, :].copy()\n",
        "\n",
        "x_test_sel_feat = extractor(x_test_sel)[feature_layer].numpy()\n",
        "print(x_test_sel.shape)\n",
        "print(x_test_sel_feat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZQSiNQ-MdDZ"
      },
      "source": [
        "# Computation of LID and Hubness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29dJMpfyFY7j"
      },
      "outputs": [],
      "source": [
        "# Helper functions to compute kernels and k-NN arrays\n",
        "\n",
        "def sim_to_dist(sim_mat, n_set_one):\n",
        "  \"\"\"\n",
        "  Converts similarity matrix to distance matrix.\n",
        "  param: n_set_one is the number of top rows whose diagonal will be set to one\n",
        "        (so that the example will not be nearerst neighbor of itself)\n",
        "  \"\"\"\n",
        "  min_sim = float(numpy.min(sim_mat[:n_set_one, :n_set_one]))\n",
        "  max_sim = float(numpy.max(sim_mat[:n_set_one, :n_set_one]))\n",
        "\n",
        "  dist_mat = 1 - (sim_mat - min_sim) / (max_sim - min_sim)\n",
        "  for i in range(n_set_one):\n",
        "    dist_mat[i][i] = 1\n",
        "  return dist_mat\n",
        "\n",
        "\n",
        "def compute_knn_arrays(dist_mat):\n",
        "  knn_dist  = numpy.sort(dist_mat)\n",
        "  knn_idx  = np.argsort(dist_mat)\n",
        "  return (knn_dist, knn_idx)\n",
        "\n",
        "\n",
        "def compute_hubness(dist_mat, k):\n",
        "  \"\"\" Measures frequency of each point being among k-NN of all rest points \"\"\"\n",
        "  knn_idx  = np.argsort(dist_mat.T, axis=1)[:, :k]\n",
        "  knn_idx = knn_idx.flatten()\n",
        "  unique, counts = numpy.unique(knn_idx, return_counts=True)\n",
        "  hub_df = pd.DataFrame({'sample_id': unique, 'knn_freq': counts})\n",
        "  all_ids = pd.DataFrame({'sample_id': list(range(dist_mat.shape[0]))})\n",
        "  hub_df = pd.merge(all_ids, hub_df, how='left', on='sample_id')\n",
        "  hub_df['hub_score'] = hub_df['knn_freq'].fillna(0.0)\n",
        "  return hub_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0KVkgc5scQq"
      },
      "outputs": [],
      "source": [
        "kernel = kernel_fn(x_train, x_test_sel, params)\n",
        "kernel = covariate2Number(kernel)\n",
        "\n",
        "kernel_adv = kernel_fn(x_train, x_test_adv_sel, params)\n",
        "kernel_adv = covariate2Number(kernel_adv)\n",
        "\n",
        "combined_kernel = numpy.concatenate((kernel_train.T, kernel.T, kernel_adv.T))\n",
        "n_training_examples =  kernel_train.shape[0]\n",
        "n_test_examples = kernel.T.shape[0]\n",
        "n_test_adv_examples = kernel_adv.T.shape[0]\n",
        "dist_mat = sim_to_dist(combined_kernel, n_training_examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avgBvI1lIhka"
      },
      "source": [
        "## LID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4gCz5LUvpPC"
      },
      "outputs": [],
      "source": [
        "k = 20 #the default value in ldim_mle\n",
        "\n",
        "knn_dist, knn_idx = compute_knn_arrays(dist_mat)\n",
        "ldim_mle = skdim.id.MLE(unbiased=False)\n",
        "# X is set as dummy to avoid error, but it is not used since the actual informaton is in precomputed_knn_arrays\n",
        "lid_values = ldim_mle.fit(X=numpy.ones((knn_dist.shape[0], 2)), y=None, precomputed_knn_arrays = (knn_dist[:, :k], knn_idx[:, :k]), smooth=False)\n",
        "lid_values = lid_values.dimension_pw_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hubness"
      ],
      "metadata": {
        "id": "3OW4ZN-bPbXt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eACn-TTPAtoZ"
      },
      "outputs": [],
      "source": [
        "k = int(dist_mat.shape[1] * 0.6) #uses a high fraction of dataset's size\n",
        "\n",
        "hub_scores = compute_hubness(dist_mat, k)['hub_score']\n",
        "hub_train_df = pd.DataFrame({'train': hub_scores[0:n_training_examples]}).reset_index(drop=True)\n",
        "hub_test_df = pd.DataFrame({'test': hub_scores[n_training_examples:(n_training_examples+n_test_examples)]}).reset_index(drop=True)\n",
        "hub_test_adv_df = pd.DataFrame({'test_adv': hub_scores[(n_training_examples+n_test_examples):]}).reset_index(drop=True)\n",
        "hub_df = pd.concat([hub_train_df, hub_test_df, hub_test_adv_df], axis=1)\n",
        "hub_df = hub_df[['train', 'test', 'test_adv']]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}